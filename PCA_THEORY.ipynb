{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a16214-9481-4d04-9a90-95a566a82aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "# Definition:\n",
    "# Principal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning. It transforms high-dimensional data into a smaller \n",
    "# number of new features called principal components, while retaining as much important information (variance) as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c6c508-816b-4e15-b63b-c95021d6e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why do we use PCA?\n",
    "\n",
    "# 1.Dimensionality Reduction: Reduces a large number of features into fewer ones while preserving patterns in the data.\n",
    "# 2.Visualization: Helps plot high-dimensional data in 2D or 3D space.\n",
    "# 3.Noise Removal: Keeps only the most important features, filtering out random noise.\n",
    "# 4.Faster Computation: Models train faster when there are fewer features.\n",
    "# 5.Avoid Multicollinearity: PCA creates uncorrelated (independent) components, avoiding redundancy in features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165dc757-8e2c-4c55-9d7e-dfd18ff7d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does PCA work?\n",
    "\n",
    "# 1.Suppose we have a dataset with many features:\n",
    "# 2.Standardize the Data: Since features may have different scales, normalize them (mean = 0, standard deviation = 1).\n",
    "# 3.Compute the Covariance Matrix: This shows how variables are related to each other.\n",
    "# 4.Find Eigenvalues and Eigenvectors:\n",
    "# 5.Eigenvectors = direction of new features (principal components).\n",
    "# 6.Eigenvalues = amount of variance (importance) carried by each component.\n",
    "# 7.Sort by Variance (Descending Order): Keep the components with the highest variance, since they carry the most important information.\n",
    "# 8.Form the New Dataset: Project the original data onto the selected principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6d858-62ec-445f-a777-9702c1417877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to select the best number of components (k) in PCA?\n",
    "\n",
    "# 1.Elbow Method: Plot eigenvalues vs. number of components. Look for the elbow point and keep components before it, \n",
    "# since additional components add very little information.\n",
    "\n",
    "# 2.Kaiser Criterion: For standardized data, keep all components with eigenvalues greater than 1. (Works best when using a correlation matrix).\n",
    "\n",
    "# 3.Cross Validation: Use PCA as preprocessing, then train the ML model. Try different values of k and choose the one giving the best validation \n",
    "# performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
